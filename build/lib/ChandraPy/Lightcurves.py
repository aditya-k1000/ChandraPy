from astropy.io import fits
from astropy.stats import bayesian_blocks, histogram
from astropy.table import Table
from astropy.time import Time
from ChandraPy import order, values
from ChandraPy import Plotting as plot
from ChandraPy import Utilities as utils
from ciao_contrib.runtool import dmextract, dmlist, dmkeypar
import matplotlib
from matplotlib.gridspec import GridSpec, GridSpecFromSubplotSpec
import matplotlib.pyplot as plt
from matplotlib.ticker import MultipleLocator
import numpy as np
import os
import pandas as pd
from scipy.special import gammaln
import warnings

def acis_lightcurve_generator(obs_dir, data_dir, source, binsize, energy_min, energy_max, remove_intermediate = True):
    """Generates ACIS light curves using 'dmextract', and returns it Pandas DataFrame.

    Args:
        obs_dir (str): Absolute path to directory where light curves are to be saved.
        data_dir (str): Absolute path to directory where data is saved. It should have name of Obs. ID, and event file should have name '{Obs. ID}_evt2.fits'.
        source (str): Name of source, preferably in J2000 sexagecimal format.
        binsize (int/float): Size of bins (s).
        energy_min (int): Lower bound of energy band (eV).
        energy_max (int): Upper bound of energy band (eV).
        remove_intermediate (bool, optional): Whether to remove the intermediate '.lc' file that's created by 'dmextract'. Defaults to True.

    Returns:
        pandas.core.frame.DataFrame: A Pandas DataFrame containing ACIS light curve generated by 'dmextract'.
    """

    obs_id = data_dir.split("/")[-1]
    event_file = os.path.join(data_dir, f"{obs_id}_evt2.fits")
    region_file = os.path.join(obs_dir, f"{source}_{obs_id}.reg")
    outfile = os.path.join(obs_dir, f"{source}.lc")
    timedel = float(dmkeypar(infile = event_file, keyword = "TIMEDEL", echo = True))

    dmextract.punlearn()
    dmextract.infile = f"{event_file}[sky=region({region_file})][energy={energy_min}:{energy_max}][bin time=::{float(binsize) // timedel * timedel}]"
    dmextract.outfile = outfile
    dmextract.opt = "ltc1"
    dmextract.clobber = "yes"
    dmextract()

    with fits.open(outfile) as hdul:
        table = Table(hdul[1].data)
        df = table.to_pandas()

    if remove_intermediate:
        os.remove(outfile)

    return df

def hrc_lightcurve_generator(obs_dir, data_dir, source, binsize, remove_intermediate = True):
    """Generates HRC light curves using 'dmextract', and returns it Pandas DataFrame.

    Args:
        obs_dir (str): Absolute path to directory where light curves are to be saved.
        data_dir (str): Absolute path to directory where data is saved. It should have name of Obs. ID, event file should have name '{Obs. ID}_evt2.fits', and dtf file should have name '{Obs. ID}_dtf1.fits'.
        source (str): Name of source, preferably in J2000 sexagecimal format.
        binsize (int/float): Size of bins (s).
        remove_intermediate (bool, optional): Whether to remove the intermediate '.lc' file that's created by 'dmextract'. Defaults to True.

    Returns:
        pandas.core.frame.DataFrame: A Pandas DataFrame containing ACIS light curve generated by 'dmextract'.
    """
        
    obs_id = data_dir.split("/")[-1]
    event_file = os.path.join(data_dir, f"{obs_id}_evt2.fits")
    dtf_file = os.path.join(data_dir, f"{obs_id}_dtf1.fits")
    region_file = os.path.join(obs_dir, f"{source}_{obs_id}.reg")
    outfile = os.path.join(obs_dir, f"{source}.lc")
    timedel = float(dmkeypar(infile = event_file, keyword = "TIMEDEL", echo = True))

    dmextract.punlearn()
    dmextract.infile = f"{event_file}[sky=region({region_file})][samp=10:300][bin time=::{float(binsize) // timedel * timedel}]"
    dmextract.outfile = outfile
    dmextract.exp = dtf_file
    dmextract.opt = "ltc1"
    dmextract.clobber = "yes"
    dmextract()

    with fits.open(outfile) as hdul:
        table = Table(hdul[1].data)
        df = table.to_pandas()

    df = pd.DataFrame({"Bin": df["TIME_BIN"], "Time": df["TIME"], "Broadband Count Rate": df["COUNT_RATE"],"Broadband Counts": df["COUNTS"], "Count Rate Error": df["COUNT_RATE_ERR"], "Exposure": df["EXPOSURE"]})
    first_valid = df[df["Exposure"] != 0].index.min()
    last_valid = df[df["Exposure"] != 0].index.max()
    df = df.loc[first_valid:last_valid].reset_index(drop = True)
    df["Bin"] = range(1, len(df) + 1)
    df = df.drop(columns = ["Exposure"])

    if remove_intermediate:
        os.remove(outfile)

    return df

def lightcurve_generation(obs_dir, data_dir, source, binsize, p0 = 5, likelihood_threshold = np.log(1e-3), seed = 1, bkg_sub = False):
    """Generates a complete set of ACIS/HRC light curves for a given source.

    Args:
        obs_dir (str): Absolute path to directory where light curves are to be saved.
        data_dir (str): Absolute path to directory where data is saved. It should have name of Obs. ID, event file should have name {Obs. ID}_evt2.fits, and dtf file should have name {Obs. ID}_dtf1.fits.
        source (str): Name of source, preferably in J2000 sexagecimal format.
        binsize (int/float): Size of bins (s).
        p0 (int/float, optional): Value of p0 for Bayesian Blocks Segmentation. Defaults to 5.
        likelihood_threshold (int/float, optional): Value of threshold to determine whether segment can be merged or not during Bayesian Segmentation. Defaults to ln(0.001).
        seed (int, optional): Seed used by numpy to add randomization to event list to space out events within frame readout times. Defaults to 1.
        bkg_sub (bool, optional): _description_. Defaults to False.

    Returns:
        bool: A boolean value that tells the user whether the set of light curves was successfully generated or not.
    """

    os.chdir(obs_dir)
    np.random.seed(seed)
    status = True
    chandra_mjd_ref = 50814.0
    obs_id = data_dir.split("/")[-1]
    event_file = os.path.join(data_dir, f"{obs_id}_evt2.fits")
    tstart = float(dmkeypar(infile = event_file, keyword = "TSTART", echo = True))
    tstop = float(dmkeypar(infile = event_file, keyword = "TSTOP", echo = True))
    instrument = utils.instrument_checker(event_file)
    
    if bkg_sub:
        if instrument == "ACIS":
            df = pd.DataFrame()
            for _, row in values.iterrows():
                band = row["Band"]
                energy_min = row["Energy Min"]
                energy_max = row["Energy Max"]

                lc_data = acis_lightcurve_generator(obs_dir, data_dir, source, binsize, energy_min, energy_max, band)

                df["Bin"] = lc_data["TIME_BIN"]
                df["Time"] = lc_data["TIME"]
                df[f"{band} Count Rate"] = lc_data["COUNT_RATE"]
                df["Bin"] = lc_data["TIME_BIN"]
                df[f"{band} Counts"] = lc_data["COUNTS"]

                if band == "Broadband":
                    df["Count Rate Error"] = lc_data["COUNT_RATE_ERR"]
                    df["Exposure"] = lc_data["EXPOSURE"]

            #Trim off 0 data
            first_valid = df[df["Exposure"] != 0].index.min()
            last_valid = df[df["Exposure"] != 0].index.max()
            df = df.loc[first_valid:last_valid].reset_index(drop = True)
            df["Bin"] = range(1, len(df) + 1)
            df["Time"] -= tstart
            df = df[order]

        else:
            df = hrc_lightcurve_generator(obs_dir, data_dir, source, binsize)
            df["Time"] -= tstart

        final_csv = os.path.join(obs_dir, f"{source}_{obs_id}.csv")
        df.to_csv(final_csv, index = False)

        if df["Broadband Counts"].sum() == 0:
            os.remove(final_csv)
            status = False
        else:
            print()
            #Add Code for plotting
    else:
        if instrument == "ACIS":
            df = pd.DataFrame()
            for _, row in values.iterrows():
                band = row["Band"]
                energy_min = row["Energy Min"]
                energy_max = row["Energy Max"]

                lc_data = acis_lightcurve_generator(obs_dir, data_dir, source, binsize, energy_min, energy_max, band)

                df["Bin"] = lc_data["TIME_BIN"]
                df["Time"] = lc_data["TIME"]
                df[f"{band} Count Rate"] = lc_data["COUNT_RATE"]
                df["Bin"] = lc_data["TIME_BIN"]
                df[f"{band} Counts"] = lc_data["COUNTS"]

                if band == "Broadband":
                    df["Count Rate Error"] = lc_data["COUNT_RATE_ERR"]
                    df["Exposure"] = lc_data["EXPOSURE"]

            first_valid = df[df["Exposure"] != 0].index.min()
            last_valid = df[df["Exposure"] != 0].index.max()
            df = df.loc[first_valid:last_valid].reset_index(drop = True)
            df["Bin"] = range(1, len(df) + 1)
            df = df[order]
        else:
            df = hrc_lightcurve_generator(obs_dir, data_dir, source, binsize)
            first_valid = df[df["Broadband Counts"] != np.nan].index.min()

        final_csv = os.path.join(obs_dir, f"{source}_{obs_id}.csv")
        df.to_csv(final_csv, index = False)

        if df["Broadband Counts"].sum() == 0:
            os.remove(final_csv)
            status = False
        else:
            start_time_days = tstart / 86400.0  
            observation_mjd = chandra_mjd_ref + start_time_days
            observation_date = Time(observation_mjd, format = "mjd").to_datetime()
            readable_date = observation_date.strftime("%B %d, %Y %I:%M:%S %p")
            observation_duration = (tstop - tstart) / 1000
            total_counts = df["Broadband Counts"].sum()
            timedel = float(dmkeypar(infile = os.path.join(data_dir, f"{obs_id}_evt2.fits"), keyword = "TIMEDEL", echo = True))
            width = 12 * (500 / (float(binsize) if 250 <= float(binsize) < 500 else (2 * float(binsize) if float(binsize) < 250 else 500)))
            matplotlib.use("agg")

            with warnings.catch_warnings():
                warnings.simplefilter("ignore", UserWarning)
                if instrument == "ACIS":
                    nrows = 6
                    fig, (postage_stamp_plot, broadband_rate_plot, bayesian_blocks_plot, broadband_counts_plot, cumulative_counts_plot, hr_plot) = plt.subplots(
                            nrows = nrows, ncols = 1, figsize = (width, nrows * 4), layout = "constrained")
                else:
                    nrows = 5
                    fig, (postage_stamp_plot, broadband_rate_plot, bayesian_blocks_plot, broadband_counts_plot, cumulative_counts_plot) = plt.subplots(
                            nrows = nrows, ncols = 1, figsize = (width, nrows * 4), layout = "constrained")
                    
                gs = GridSpec(nrows + 1, 1, figure = fig)
                
                plt.sca(broadband_rate_plot)
                if instrument == "ACIS":
                    broadband_rate_plot.plot((df["Time"] - tstart) / 1000, df["Broadband Count Rate"] * timedel, color = "gray", linewidth = 0.75)
                    plot.rate_plotter(plt, np.array((df["Time"] - tstart) / 1000), df["Broadband Count Rate"], "black", df["Count Rate Error"], timedel)
                else:
                    broadband_rate_plot.plot((df["Time"] - tstart) / 1000, df["Broadband Count Rate"], color = "gray", linewidth = 0.75)
                    plot.rate_plotter(plt, np.array((df["Time"] - tstart) / 1000), df["Broadband Count Rate"], "black", df["Count Rate Error"])

                rate_range = max(df["Broadband Count Rate"] + df["Count Rate Error"]) - min(df["Broadband Count Rate"] - df["Count Rate Error"])
                broadband_rate_plot.set_title("(A) Broadband Count Rate", fontsize = 14, y = 1.04)
                broadband_rate_plot.set_xlim([0, observation_duration])
                broadband_rate_plot.set_ylim(bottom = 0 - 0.025 * rate_range)
                broadband_rate_plot.set_xlabel(f"Time +{(tstart / 1000):.4f} (ks)")
                broadband_rate_plot.yaxis.set_label_coords(-0.045, 0.5)
                broadband_rate_plot.xaxis.set_major_locator(MultipleLocator(5))
                broadband_rate_plot.xaxis.set_minor_locator(MultipleLocator(1))
                broadband_rate_plot.text(0.005, 1.135, f"Source: {source}\nObs. ID: {obs_id}", transform = broadband_rate_plot.transAxes, fontsize = 11.5, ha = "left", va = "top", bbox = dict(facecolor = "white", linewidth = 0.85))
                broadband_rate_plot.text(0.995, 1.135, f"Start: {readable_date}\nTimeDel: {timedel}", transform = broadband_rate_plot.transAxes, fontsize = 11.5, ha = "right", va = "top", multialignment = "left", bbox = dict(facecolor = "white", linewidth = 0.85))

                plt.sca(bayesian_blocks_plot)
                bb_dict = {}
                if instrument == "ACIS":
                    region_event_file, instrument = utils.isolate_source_region(obs_dir, data_dir, source)
                    sky_image, detector_image = utils.create_postage_stamps(obs_dir, source, region_event_file, event_file, 256, 128)
                    dmlist(infile = f"{region_event_file}[cols time,energy]", outfile = f"{region_event_file}.txt", opt = "data,raw")
                    broadband_event_list = Table.read(filename = f"{region_event_file}.txt", format = "ascii").to_pandas()
                    broadband_event_list.rename(columns = {"time": "Time", "energy": "Energy"}, inplace = True)
                    broadband_event_list["Time"] -= tstart
                    broadband_event_list["Time"] += np.random.uniform(0, timedel, size = len(broadband_event_list["Time"]))
                    bin_edges = np.array(bayesian_blocks(broadband_event_list["Time"], fitness = "events", p0 = p0))
                    bin_edges[-1] = tstop - tstart
                    counts_bb, _ = histogram(broadband_event_list["Time"], bin_edges)
                    time_intervals = np.diff(bin_edges)
                    count_rates = counts_bb / time_intervals

                    i = 0
                    while i < len(time_intervals):
                        if (time_intervals[i] < 20 or counts_bb[i] < 5):
                            if i == 0:
                                if count_rates[i] > count_rates[i + 1] and not counts_bb[i] > counts_bb[i + 1]:
                                    counts_bb[i + 1] += counts_bb[i]
                                    counts_bb = np.delete(counts_bb, i)
                                    bin_edges = np.delete(bin_edges, i + 1)
                                else:
                                    i += 1
                                    continue
                            elif i == len(time_intervals) - 1:
                                if count_rates[i] > count_rates[i - 1] and not counts_bb[i] > counts_bb[i - 1]:
                                    counts_bb[i - 1] += counts_bb[i]
                                    counts_bb = np.delete(counts_bb, i)
                                    bin_edges = np.delete(bin_edges, i)
                                    i -= 1
                                else:
                                    break
                            else:
                                right_diff = count_rates[i + 1] - count_rates[i]
                                left_diff = count_rates[i - 1] - count_rates[i]
                                if count_rates[i] > count_rates[i - 1] and count_rates[i] > count_rates[i + 1] and not (counts_bb[i] > counts_bb[i - 1] and counts_bb[i] > counts_bb[i + 1]):
                                    if abs(left_diff) < abs(right_diff):
                                        counts_bb[i - 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i)
                                        i -= 1
                                    else:
                                        counts_bb[i + 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i + 1)
                                        i += 1
                                else:
                                    i += 1
                                    continue
                        else:
                            i += 1
                            continue

                        time_intervals = np.diff(bin_edges)
                        count_rates = counts_bb / time_intervals

                    i = 0
                    while i < len(time_intervals):
                        if time_intervals[i] < 100:
                            if i == 0:
                                if count_rates[i] > count_rates[i + 1]:
                                    ln_next_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i + 1] * time_intervals[i])) - (count_rates[i + 1] * time_intervals[i])
                                    if not ln_next_likelihood < likelihood_threshold:
                                        counts_bb[i + 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i + 1)
                                    else:
                                        i += 1
                                        continue
                                else:
                                    i += 1
                                    continue

                            elif i == len(time_intervals) - 1:
                                if count_rates[i] > count_rates[i - 1]:
                                    ln_previous_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i - 1] * time_intervals[i])) - (count_rates[i - 1] * time_intervals[i])
                                    if not ln_previous_likelihood < likelihood_threshold:
                                        counts_bb[i - 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i)
                                        i -= 1
                                    else:
                                        break
                                else:
                                    break

                            else:
                                if count_rates[i] > count_rates[i - 1] and count_rates[i] > count_rates[i + 1]:
                                    ln_next_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i + 1] * time_intervals[i])) - (count_rates[i + 1] * time_intervals[i])
                                    ln_previous_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i - 1] * time_intervals[i])) - (count_rates[i - 1] * time_intervals[i])
                                    if not np.max([ln_previous_likelihood, ln_next_likelihood]) < likelihood_threshold:
                                        if ln_previous_likelihood > ln_next_likelihood:
                                            counts_bb[i - 1] += counts_bb[i]
                                            counts_bb = np.delete(counts_bb, i)
                                            bin_edges = np.delete(bin_edges, i)
                                            i -= 1
                                        else:
                                            counts_bb[i + 1] += counts_bb[i]
                                            counts_bb = np.delete(counts_bb, i)
                                            bin_edges = np.delete(bin_edges, i + 1)
                                            i += 1
                                    else:    
                                        i += 1
                                        continue
                                else:
                                    i += 1
                                    continue

                            time_intervals = np.diff(bin_edges)
                            count_rates = counts_bb / time_intervals
                            
                        else:
                            i += 1
                            continue

                    bb_dict["Bin Edges"] = bin_edges
                    bb_dict["Time Intervals"] = time_intervals

                    os.remove(region_event_file)
                    os.remove(f"{region_event_file}.txt")

                    for _, row in values.iterrows():
                        band = row["Band"]
                        energy_min = row["Energy Min"]
                        energy_max = row["Energy Max"]
                        color = row["Color"]
                        text = row["Text"]

                        if band == "Broadband":
                            counts_bb, count_rates_bb = plot.bayesian_blocks_plotter(plt, broadband_event_list["Time"], bin_edges, color, text, True)
                        else:
                            counts_bb, count_rates_bb = plot.bayesian_blocks_plotter(plt, broadband_event_list["Time"][(broadband_event_list["Energy"] >= energy_min) & (broadband_event_list["Energy"] < energy_max)], bin_edges, color, text, False, 0.6)
                                
                        bb_dict[f"{band} Counts"] = counts_bb
                        bb_dict[f"{band} Count Rate"] = count_rates_bb

                        bayesian_blocks_plot.set_title(fr"(B) Bayesian Blocks Segmentation: $p_0 = {p0}$", fontsize = 14, y = 1.07)
                        bayesian_blocks_plot.legend(loc = "upper center", bbox_to_anchor = (0.5, 1.0985), ncol = 5, frameon = False, fontsize = 8.35, columnspacing = 1)

                else:
                    tstart = float(dmkeypar(infile = event_file, keyword = "TSTART", echo = True))
                    tstop = float(dmkeypar(infile = event_file, keyword = "TSTOP", echo = True))
                    region_event_file, instrument = utils.isolate_source_region(obs_dir, data_dir, source, 100, 10000)
                    sky_image, detector_image = utils.create_postage_stamps(obs_dir, source, region_event_file, event_file, 256, 128)
                    dmlist(infile = f"{region_event_file}[cols time,pi]", outfile = f"{region_event_file}.txt", opt = "data,raw")
                    broadband_event_list = Table.read(filename = f"{region_event_file}.txt", format = "ascii").to_pandas()
                    broadband_event_list.rename(columns = {"time": "Time", "pi": "PI"}, inplace = True)
                    broadband_event_list["Time"] -= tstart
                    broadband_event_list["Time"] += np.random.uniform(0, timedel, size = len(broadband_event_list["Time"]))
                    bin_edges = np.array(bayesian_blocks(broadband_event_list["Time"], fitness = "events", p0 = p0))
                    bin_edges[-1] = tstop - tstart
                    counts_bb, _ = histogram(broadband_event_list["Time"], bin_edges)
                    time_intervals = np.diff(bin_edges)
                    count_rates = counts_bb / time_intervals

                    i = 0
                    while i < len(time_intervals):
                        if (time_intervals[i] < 20 or counts_bb[i] < 5):
                            if i == 0:
                                if count_rates[i] > count_rates[i + 1] and not counts_bb[i] > counts_bb[i + 1]:
                                    counts_bb[i + 1] += counts_bb[i]
                                    counts_bb = np.delete(counts_bb, i)
                                    bin_edges = np.delete(bin_edges, i + 1)
                                else:
                                    i += 1
                                    continue
                            elif i == len(time_intervals) - 1:
                                if count_rates[i] > count_rates[i - 1] and not counts_bb[i] > counts_bb[i - 1]:
                                    counts_bb[i - 1] += counts_bb[i]
                                    counts_bb = np.delete(counts_bb, i)
                                    bin_edges = np.delete(bin_edges, i)
                                    i -= 1
                                else:
                                    break
                            else:
                                right_diff = count_rates[i + 1] - count_rates[i]
                                left_diff = count_rates[i - 1] - count_rates[i]
                                if count_rates[i] > count_rates[i - 1] and count_rates[i] > count_rates[i + 1] and not (counts_bb[i] > counts_bb[i - 1] and counts_bb[i] > counts_bb[i + 1]):
                                    if abs(left_diff) < abs(right_diff):
                                        counts_bb[i - 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i)
                                        i -= 1
                                    else:
                                        counts_bb[i + 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i + 1)
                                        i += 1
                                else:
                                    i += 1
                                    continue
                        else:
                            i += 1
                            continue

                        time_intervals = np.diff(bin_edges)
                        count_rates = counts_bb / time_intervals

                    i = 0
                    while i < len(time_intervals):
                        if time_intervals[i] < 100:
                            if i == 0:
                                if count_rates[i] > count_rates[i + 1]:
                                    ln_next_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i + 1] * time_intervals[i])) - (count_rates[i + 1] * time_intervals[i])
                                    if not ln_next_likelihood < likelihood_threshold:
                                        counts_bb[i + 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i + 1)
                                    else:
                                        i += 1
                                        continue
                                else:
                                    i += 1
                                    continue

                            elif i == len(time_intervals) - 1:
                                if count_rates[i] > count_rates[i - 1]:
                                    ln_previous_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i - 1] * time_intervals[i])) - (count_rates[i - 1] * time_intervals[i])
                                    if not ln_previous_likelihood < likelihood_threshold:
                                        counts_bb[i - 1] += counts_bb[i]
                                        counts_bb = np.delete(counts_bb, i)
                                        bin_edges = np.delete(bin_edges, i)
                                        i -= 1
                                    else:
                                        break
                                else:
                                    break

                            else:
                                if count_rates[i] > count_rates[i - 1] and count_rates[i] > count_rates[i + 1]:
                                    ln_next_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i + 1] * time_intervals[i])) - (count_rates[i + 1] * time_intervals[i])
                                    ln_previous_likelihood = (-1 * gammaln(counts_bb[i] + 1)) + ((counts_bb[i]) * np.log(count_rates[i - 1] * time_intervals[i])) - (count_rates[i - 1] * time_intervals[i])
                                    if not np.max([ln_previous_likelihood, ln_next_likelihood]) < likelihood_threshold:
                                        if ln_previous_likelihood > ln_next_likelihood:
                                            counts_bb[i - 1] += counts_bb[i]
                                            counts_bb = np.delete(counts_bb, i)
                                            bin_edges = np.delete(bin_edges, i)
                                            i -= 1
                                        else:
                                            counts_bb[i + 1] += counts_bb[i]
                                            counts_bb = np.delete(counts_bb, i)
                                            bin_edges = np.delete(bin_edges, i + 1)
                                            i += 1
                                    else:    
                                        i += 1
                                        continue
                                else:
                                    i += 1
                                    continue

                            time_intervals = np.diff(bin_edges)
                            count_rates = counts_bb / time_intervals

                        else:
                            i += 1
                            continue

                    time_intervals = np.diff(bin_edges)
                    count_rates = counts_bb / time_intervals
                    counts_bb, count_rates_bb = plot.bayesian_blocks_plotter(plt, broadband_event_list["Time"], bin_edges, "black", "", True)

                    bb_dict["Bin Edges"] = bin_edges
                    bb_dict["Time Intervals"] = time_intervals
                    bb_dict["Broadband Counts"] = counts_bb
                    bb_dict["Broadband Count Rate"] = count_rates_bb

                    os.remove(region_event_file)
                    os.remove(f"{region_event_file}.txt")

                    bayesian_blocks_plot.set_title(fr"(B) Bayesian Blocks Segmentation: $p_0 = {p0}$", fontsize = 14, y = 1.04)
                        
                bayesian_blocks_plot.set_xlim([0, observation_duration])
                bayesian_blocks_plot.set_ylim(bottom = 0 - 0.05 * max(bb_dict["Broadband Count Rate"]))
                bayesian_blocks_plot.yaxis.set_label_coords(-0.045, 0.5)
                bayesian_blocks_plot.set_xlabel(f"Time +{(tstart / 1000):.4f} (ks)")
                bayesian_blocks_plot.set_ylabel(f"Count Rate over {(df['Time'].iloc[1] - df['Time'].iloc[0]):.2f}s bins (cts/s)")
                bayesian_blocks_plot.xaxis.set_major_locator(MultipleLocator(5))
                bayesian_blocks_plot.xaxis.set_minor_locator(MultipleLocator(1))

                max_len = max(len(v) for v in bb_dict.values())

                for key, arr in bb_dict.items():
                    arr = np.asarray(arr, dtype = float)
                    pad_width = max_len - len(arr)
                    padded = np.pad(arr, (pad_width, 0), mode = "constant", constant_values = np.nan)
                    bb_dict[key] = padded

                bb_df = pd.DataFrame(bb_dict)
                bb_df.to_csv(os.path.join(obs_dir, f"{source}_{obs_id}_bb.csv"), index = False)

                plt.sca(broadband_counts_plot)
                plot.counts_plotter(plt, np.array((df["Time"] - tstart) / 1000), df["Broadband Counts"], "purple")
                counts_range = max(df["Broadband Counts"]) - min(df["Broadband Counts"])
                broadband_counts_plot.set_title("(C) Broadband Counts", fontsize = 14, y = 1.04)
                broadband_counts_plot.set_xlim([0, observation_duration])
                broadband_counts_plot.set_ylim(bottom = 0 - 0.025 * counts_range)
                broadband_counts_plot.set_xlabel(f"Time +{(tstart / 1000):.4f} (ks)")
                broadband_counts_plot.yaxis.set_label_coords(-0.045, 0.5)
                broadband_counts_plot.xaxis.set_major_locator(MultipleLocator(5))  
                broadband_counts_plot.xaxis.set_minor_locator(MultipleLocator(1))
                average_count_rate = round(float(df["Broadband Count Rate"].mean()) * 1000, 3)
                min_count_rate = round(float(df["Broadband Count Rate"].min()) * 1000, 3)
                max_count_rate = round(float(df["Broadband Count Rate"].max()) * 1000, 3)

                left_text_1 = f"Total Counts: {total_counts:.3f}"
                right_text_1 = f"Avg. CR (cts/ks): {average_count_rate:.3f}"
                left_text_2 = f"Min. CR (cts/ks): {min_count_rate:.3f}"
                right_text_2 = f"Max. CR (cts/ks): {max_count_rate:.3f}"

                max_left_length = max(len(left_text_1), len(left_text_2))
                max_right_length = max(len(right_text_1), len(right_text_2))

                total_width = max_left_length + max_right_length + 5
                text_str = (f"{left_text_1:<{total_width - max_right_length}}{right_text_1}\n"f"{left_text_2:<{total_width - max_right_length}}{right_text_2}")
                broadband_counts_plot.text(0.005, 1.135, text_str, transform = broadband_counts_plot.transAxes, fontsize = 11.5, ha = "left", va = "top", bbox = dict(facecolor = "white", linewidth = 0.85))

                plt.sca(cumulative_counts_plot)
                cumulative_counts_plot.plot([(broadband_event_list["Time"] / 1000).iloc[0], (broadband_event_list["Time"] / 1000).iloc[-1]], [0, total_counts], color = "black", linewidth = 0.75, alpha = 0.7)
                plot.cumulative_counts_plotter(plt, np.array(broadband_event_list["Time"] / 1000))
 
                cumulative_counts_plot.set_title("(D) Cumulative Counts", fontsize = 14, y = 1.04)
                cumulative_counts_plot.set_xlim([0, observation_duration])
                cumulative_counts_plot.set_ylim(bottom = 0)
                cumulative_counts_plot.yaxis.set_label_coords(-0.045, 0.5)
                cumulative_counts_plot.set_xlabel(f"Time +{(tstart / 1000):.4f} (ks)")
                cumulative_counts_plot.xaxis.set_major_locator(MultipleLocator(5))
                cumulative_counts_plot.xaxis.set_minor_locator(MultipleLocator(1))
                
                if instrument == "ACIS":
                    plt.sca(hr_plot)
                    plot.hr_plotter(plt, final_csv, tstart)
                    hr_plot.set_title("(E) Hardness Ratios", fontsize = 14, y = 1.07)
                    hr_plot.set_xlim([0, observation_duration])
                    hr_plot.set_ylim([-1.05, 1.05])
                    hr_plot.set_xlabel(f"Time +{(tstart / 1000):.4f} (ks)")
                    hr_plot.yaxis.set_label_coords(-0.045, 0.5)
                    hr_plot.xaxis.set_major_locator(MultipleLocator(5))
                    hr_plot.xaxis.set_minor_locator(MultipleLocator(1))
                    hr_plot.legend(loc = "upper center", bbox_to_anchor = (0.5, 1.109), ncol = 4, frameon = False, fontsize = 8.35)

                plt.sca(postage_stamp_plot)
                _, _, off_axis_angle, _, _, _ = utils.retrieve_obs_info(event_file)
                postage_stamp_plot.text(0.5, 1.015, f"Off Axis Angle: {off_axis_angle}'", transform = postage_stamp_plot.transAxes, fontsize = 12, ha = "center")
                inner_gs = GridSpecFromSubplotSpec(1, 2, subplot_spec = gs[0])
                sky_image_plot = fig.add_subplot(inner_gs[0])
                detector_image_plot = fig.add_subplot(inner_gs[1])
                plot.plot_postage_stamps(obs_dir, obs_id, source, sky_image, detector_image, instrument, off_axis_angle, (sky_image_plot, detector_image_plot))
                postage_stamp_plot.axis("off")

                fig.suptitle(f"{instrument} Lightcurve (Binsize of {float(binsize) // timedel * timedel:.2f}s)", fontsize = "xx-large")
                fig.savefig(os.path.join(obs_dir, f"{source}_{obs_id}.svg"), bbox_inches = "tight")
                plt.close(fig)

        return status 